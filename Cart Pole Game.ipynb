{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5317ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01496c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c06af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922f75bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d196707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TimeLimit.reset of <TimeLimit<CartPoleEnv<CartPole-v0>>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f75b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "physical_devices=tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b899fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.95 #discount factor\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_decay=0.995\n",
    "        self.minn_epsilon=0.01\n",
    "        self.learning_rate=0.001\n",
    "        self.model=self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        model=keras.Sequential()\n",
    "        model.add(keras.layers.Dense(units=24,input_shape=[self.state_size],activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dense(units=24,activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dense(units=2,activation=tf.keras.activations.linear))\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def act(self,state):\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    def train(self,batch_size):\n",
    "        mini_batch=random.sample(self.memory,batch_size)\n",
    "        for exp in mini_batch:\n",
    "            state,action,reward,next_state,done=exp\n",
    "            if not done:\n",
    "                target=reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target=reward\n",
    "            target_f=self.model.predict(state)\n",
    "            target_f[0][action]=target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "        if self.epsilon>self.minn_epsilon:\n",
    "            self.epsilon*=self.epsilon_decay\n",
    "            \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e931107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_epi=1000\n",
    "output_dir=\"model_weights/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e492030",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(state_size=4,action_size=2)\n",
    "done=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d29abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game episode: 0/1000 Score: 22\n",
      "game episode: 1/1000 Score: 15\n",
      "game episode: 2/1000 Score: 77\n",
      "game episode: 3/1000 Score: 21\n",
      "game episode: 4/1000 Score: 37\n",
      "game episode: 5/1000 Score: 37\n",
      "game episode: 6/1000 Score: 18\n",
      "game episode: 7/1000 Score: 15\n",
      "game episode: 8/1000 Score: 13\n",
      "game episode: 9/1000 Score: 31\n",
      "game episode: 10/1000 Score: 16\n",
      "game episode: 11/1000 Score: 27\n",
      "game episode: 12/1000 Score: 11\n",
      "game episode: 13/1000 Score: 17\n",
      "game episode: 14/1000 Score: 24\n",
      "game episode: 15/1000 Score: 19\n",
      "game episode: 16/1000 Score: 12\n",
      "game episode: 17/1000 Score: 30\n",
      "game episode: 18/1000 Score: 23\n",
      "game episode: 19/1000 Score: 24\n",
      "game episode: 20/1000 Score: 10\n",
      "game episode: 21/1000 Score: 13\n",
      "game episode: 22/1000 Score: 12\n",
      "game episode: 23/1000 Score: 14\n",
      "game episode: 24/1000 Score: 44\n",
      "game episode: 25/1000 Score: 11\n",
      "game episode: 26/1000 Score: 17\n",
      "game episode: 27/1000 Score: 18\n",
      "game episode: 28/1000 Score: 20\n",
      "game episode: 29/1000 Score: 23\n",
      "game episode: 30/1000 Score: 27\n",
      "game episode: 31/1000 Score: 24\n",
      "game episode: 32/1000 Score: 22\n",
      "game episode: 33/1000 Score: 11\n",
      "game episode: 34/1000 Score: 11\n",
      "game episode: 35/1000 Score: 36\n",
      "game episode: 36/1000 Score: 21\n",
      "game episode: 37/1000 Score: 23\n",
      "game episode: 38/1000 Score: 35\n",
      "game episode: 39/1000 Score: 25\n",
      "game episode: 40/1000 Score: 12\n",
      "game episode: 41/1000 Score: 16\n",
      "game episode: 42/1000 Score: 50\n",
      "game episode: 43/1000 Score: 30\n",
      "game episode: 44/1000 Score: 11\n",
      "game episode: 45/1000 Score: 15\n",
      "game episode: 46/1000 Score: 44\n",
      "game episode: 47/1000 Score: 15\n",
      "game episode: 48/1000 Score: 15\n",
      "game episode: 49/1000 Score: 16\n",
      "game episode: 50/1000 Score: 22\n",
      "game episode: 51/1000 Score: 17\n",
      "game episode: 52/1000 Score: 11\n",
      "game episode: 53/1000 Score: 12\n",
      "game episode: 54/1000 Score: 14\n",
      "game episode: 55/1000 Score: 13\n",
      "game episode: 56/1000 Score: 22\n",
      "game episode: 57/1000 Score: 10\n",
      "game episode: 58/1000 Score: 52\n",
      "game episode: 59/1000 Score: 15\n",
      "game episode: 60/1000 Score: 90\n",
      "game episode: 61/1000 Score: 10\n",
      "game episode: 62/1000 Score: 13\n",
      "game episode: 63/1000 Score: 10\n",
      "game episode: 64/1000 Score: 26\n",
      "game episode: 65/1000 Score: 13\n",
      "game episode: 66/1000 Score: 9\n",
      "game episode: 67/1000 Score: 29\n",
      "game episode: 68/1000 Score: 9\n",
      "game episode: 69/1000 Score: 42\n",
      "game episode: 70/1000 Score: 16\n",
      "game episode: 71/1000 Score: 11\n",
      "game episode: 72/1000 Score: 8\n",
      "game episode: 73/1000 Score: 46\n",
      "game episode: 74/1000 Score: 8\n",
      "game episode: 75/1000 Score: 40\n",
      "game episode: 76/1000 Score: 26\n",
      "game episode: 77/1000 Score: 119\n",
      "game episode: 78/1000 Score: 37\n",
      "game episode: 79/1000 Score: 10\n",
      "game episode: 80/1000 Score: 27\n",
      "game episode: 81/1000 Score: 156\n",
      "game episode: 82/1000 Score: 27\n",
      "game episode: 83/1000 Score: 11\n",
      "game episode: 84/1000 Score: 43\n",
      "game episode: 85/1000 Score: 71\n",
      "game episode: 86/1000 Score: 90\n",
      "game episode: 87/1000 Score: 36\n",
      "game episode: 88/1000 Score: 47\n",
      "game episode: 89/1000 Score: 36\n",
      "game episode: 90/1000 Score: 49\n",
      "game episode: 91/1000 Score: 17\n",
      "game episode: 92/1000 Score: 55\n",
      "game episode: 93/1000 Score: 17\n",
      "game episode: 94/1000 Score: 30\n",
      "game episode: 95/1000 Score: 57\n",
      "game episode: 96/1000 Score: 25\n",
      "game episode: 97/1000 Score: 39\n",
      "game episode: 98/1000 Score: 27\n",
      "game episode: 99/1000 Score: 81\n",
      "game episode: 100/1000 Score: 18\n",
      "game episode: 101/1000 Score: 10\n",
      "game episode: 102/1000 Score: 36\n",
      "game episode: 103/1000 Score: 61\n",
      "game episode: 104/1000 Score: 16\n",
      "game episode: 105/1000 Score: 39\n",
      "game episode: 106/1000 Score: 38\n",
      "game episode: 107/1000 Score: 37\n",
      "game episode: 108/1000 Score: 36\n",
      "game episode: 109/1000 Score: 30\n",
      "game episode: 110/1000 Score: 35\n",
      "game episode: 111/1000 Score: 17\n",
      "game episode: 112/1000 Score: 10\n",
      "game episode: 113/1000 Score: 9\n",
      "game episode: 114/1000 Score: 9\n",
      "game episode: 115/1000 Score: 11\n",
      "game episode: 116/1000 Score: 16\n",
      "game episode: 117/1000 Score: 23\n",
      "game episode: 118/1000 Score: 18\n",
      "game episode: 119/1000 Score: 21\n",
      "game episode: 120/1000 Score: 31\n",
      "game episode: 121/1000 Score: 14\n",
      "game episode: 122/1000 Score: 21\n",
      "game episode: 123/1000 Score: 69\n",
      "game episode: 124/1000 Score: 17\n",
      "game episode: 125/1000 Score: 24\n",
      "game episode: 126/1000 Score: 133\n",
      "game episode: 127/1000 Score: 70\n",
      "game episode: 128/1000 Score: 70\n",
      "game episode: 129/1000 Score: 35\n",
      "game episode: 130/1000 Score: 35\n",
      "game episode: 131/1000 Score: 41\n",
      "game episode: 132/1000 Score: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x000001D804085B80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pranjal bhatt\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 545, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"C:\\Users\\pranjal bhatt\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1262, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game episode: 133/1000 Score: 33\n",
      "game episode: 134/1000 Score: 19\n",
      "game episode: 135/1000 Score: 44\n",
      "game episode: 136/1000 Score: 16\n",
      "game episode: 137/1000 Score: 58\n",
      "game episode: 138/1000 Score: 33\n",
      "game episode: 139/1000 Score: 17\n",
      "game episode: 140/1000 Score: 11\n",
      "game episode: 141/1000 Score: 25\n",
      "game episode: 142/1000 Score: 25\n",
      "game episode: 143/1000 Score: 47\n",
      "game episode: 144/1000 Score: 19\n",
      "game episode: 145/1000 Score: 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2f72cca6255e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"weights_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'{:04d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-e31659a8f549>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range (no_epi):\n",
    "    state=env.reset()\n",
    "    state=np.reshape(state,(1,4))\n",
    "    sum1=0\n",
    "    for t in range(5000):\n",
    "        env.render()\n",
    "        action=agent.act(state)\n",
    "        next_state,reward,done,other_info=env.step(action)\n",
    "        reward=reward if not done else -10\n",
    "        next_state=np.reshape(next_state,(1,4))\n",
    "        sum1=sum1+reward\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(\"game episode: {}/{} Score: {}\".format(e,no_epi,t))\n",
    "            break\n",
    "    if len(agent.memory)>32:\n",
    "        agent.train(32)\n",
    "    if e%50==0:\n",
    "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+'.hdf5')\n",
    "\n",
    "print(\"deep q learner trained\")\n",
    "env.close()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad46dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
